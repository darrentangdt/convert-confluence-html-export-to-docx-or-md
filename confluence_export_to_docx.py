import os
import shutil
import subprocess
import argparse
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import unquote
from packaging.version import parse as parse_version

# ---- Default Config ----
DEFAULT_EXPORT_ROOT = "Exported_Space"
DEFAULT_OUTPUT_ROOT = "Output"
ASSET_DIRS = ["images", "attachments", "styles"]
PANDOC = "pandoc"
DEFAULT_OUTPUT_TYPE = "markdown"
# -----------------------

def print_usage_example():
    """æ‰“å°ä½¿ç”¨ç¤ºä¾‹"""
    print("\nä½¿ç”¨ç¤ºä¾‹:")
    print("1. è½¬æ¢ä¸ºMarkdown(é»˜è®¤):")
    print("   python convert_confluence.py --export-root MyConfluenceExport --output-root MyOutput")
    print("\n2. è½¬æ¢ä¸ºDOCX:")
    print("   python convert_confluence.py --type docx --export-root MyConfluenceExport --output-root MyOutput")
    print("\n3. è·³è¿‡HTMLç”Ÿæˆ(ä»…è½¬æ¢):")
    print("   python convert_confluence.py --skip-html --export-root MyConfluenceExport --output-root MyOutput")
    print("\n4. è½¬æ¢åæ¸…ç†ä¸­é—´æ–‡ä»¶:")
    print("   python convert_confluence.py --cleanup --export-root MyConfluenceExport --output-root MyOutput")
    print("\n5. æ˜¾ç¤ºå®Œæ•´å¸®åŠ©:")
    print("   python convert_confluence.py --help")

def get_pandoc_version():
    """è·å–Pandocç‰ˆæœ¬å·"""
    try:
        result = subprocess.run([PANDOC, "--version"], 
                              capture_output=True, 
                              text=True,
                              check=True)
        first_line = result.stdout.split('\n')[0]
        version_str = first_line.split()[1]
        return parse_version(version_str)
    except Exception as e:
        print(f"âš  è­¦å‘Š: æ— æ³•è·å–Pandocç‰ˆæœ¬ ({e})")
        return parse_version("0.0.0")

def sanitize(name):
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    return "".join(c if c.isalnum() or c in " -._" else "_" for c in name).strip()

def walk_index(ul, base_path):
    """éå†Confluenceç´¢å¼•æ–‡ä»¶"""
    page_map = {}
    for li in ul.find_all("li", recursive=False):
        a = li.find("a")
        if a and "href" in a.attrs:
            href = unquote(a["href"])
            title = sanitize(a.get_text(strip=True))
            rel_path = base_path / title
            page_map[href] = {
                'path': rel_path,
                'title': title
            }
            for child_ul in li.find_all("ul", recursive=False):
                page_map.update(walk_index(child_ul, rel_path))
    return page_map

def rewrite_assets(soup: BeautifulSoup, output_path: Path, format_type: str):
    """é‡å†™èµ„æºæ–‡ä»¶è·¯å¾„"""
    for tag, attr in [("img", "src"), ("link", "href"), ("script", "src"), ("a", "href")]:
        for el in soup.find_all(tag):
            src = el.get(attr)
            if not src:
                continue
            for asset_dir in ASSET_DIRS:
                if src.startswith(asset_dir + "/"):
                    abs_asset_path = EXPORT_ROOT / src
                    if format_type == 'docx':
                        rel_path = os.path.relpath(abs_asset_path, start=output_path.parent)
                    else:  # MDæ ¼å¼
                        rel_path = os.path.relpath(abs_asset_path, start=EXPORT_ROOT)
                        rel_path = str(Path('assets') / rel_path.split('/')[-1])
                    el[attr] = rel_path.replace(os.sep, "/")
                    break

def build_structure_and_rewrite_links(page_map, format_type):
    """æ„å»ºç›®å½•ç»“æ„å¹¶é‡å†™é“¾æ¥"""
    rewritten_html_paths = []
    for html_filename, page_info in page_map.items():
        src_path = EXPORT_ROOT / html_filename
        if not src_path.exists():
            print(f"âš  ç¼ºå°‘æºHTMLæ–‡ä»¶: {html_filename}")
            continue

        dest_html_path = OUTPUT_ROOT / page_info['path'].with_suffix(".html")
        dest_html_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            shutil.copy2(src_path, dest_html_path)

            with open(dest_html_path, "r", encoding="utf-8") as f:
                soup = BeautifulSoup(f, "html.parser")

            # æ¸…ç†ä¸éœ€è¦çš„å…ƒç´ 
            for element in [
                "div#breadcrumb-section",
                "h2#attachments",
                "#main-content > div.plugin-tabmeta-details",
                "h1#title-heading",
                "div#footer",
                lambda x: x and x.startswith('expander-')
            ]:
                if isinstance(element, str):
                    el = soup.select_one(element)
                    if el:
                        el.decompose()
                else:
                    for el in soup.find_all('div', id=element):
                        el.decompose()

            # è½¬æ¢å›¾ç‰‡ä¸ºMarkdownæ ¼å¼
            if format_type == 'markdown':
                for img_wrapper in soup.find_all('span', class_='confluence-embedded-file-wrapper'):
                    img = img_wrapper.find('img')
                    if img and img.has_attr('src'):
                        alt_text = img.get('alt', '')
                        img_src = img['src']
                        md_img = f'![{alt_text}]({img_src})'
                        img_wrapper.replace_with(md_img)

            # é‡å†™å†…éƒ¨é“¾æ¥
            for a in soup.find_all("a", href=True):
                href = unquote(a["href"])
                if href.endswith(".html") and href in page_map:
                    target_path = page_map[href]['path']
                    rel_path = os.path.relpath(
                        OUTPUT_ROOT / target_path.with_suffix('.md' if format_type == 'markdown' else '.docx'),
                        start=dest_html_path.parent
                    )
                    a["href"] = rel_path.replace(os.sep, "/")

            rewrite_assets(soup, dest_html_path, format_type)

            with open(dest_html_path, "w", encoding="utf-8") as f:
                f.write(str(soup))

            rewritten_html_paths.append(dest_html_path)
            print(f"âœ“ å·²é‡å†™: {html_filename} â†’ {dest_html_path.relative_to(OUTPUT_ROOT)}")

        except Exception as e:
            print(f"âœ— é‡å†™å¤±è´¥ {html_filename}: {e}")
    return rewritten_html_paths

def copy_asset_dirs(format_type):
    """å¤åˆ¶èµ„æºç›®å½•"""
    for asset_dir in ASSET_DIRS:
        src = EXPORT_ROOT / asset_dir
        if format_type == 'markdown':
            dst = OUTPUT_ROOT / 'assets'
        else:
            dst = OUTPUT_ROOT / asset_dir
            
        if src.exists():
            shutil.copytree(src, dst, dirs_exist_ok=True)
            print(f"âœ“ å·²å¤åˆ¶èµ„æºç›®å½•: {asset_dir} â†’ {dst.relative_to(OUTPUT_ROOT)}")

def convert_to_format(input_path: Path, format_type: str):
    """å®Œå…¨å…¼å®¹çš„æ ¼å¼è½¬æ¢å‡½æ•°"""
    output_path = input_path.with_suffix(".docx" if format_type == 'docx' else '.md')
    
    cmd = [
        PANDOC,
        "-o", str(output_path.name),
        "--to", "gfm" if format_type == 'markdown' else "docx",
        "--wrap=auto",
        str(input_path.name)
    ]

    # æ·»åŠ Luaè¿‡æ»¤å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    script_dir = Path(__file__).parent
    lua_filter = script_dir / "image-fullsize.lua"
    if lua_filter.exists():
        cmd.insert(1, f"--lua-filter={lua_filter}")

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=input_path.parent
        )
        if result.returncode == 0:
            if format_type == 'markdown':
                post_process_markdown_images(output_path)
            print(f"âœ“ å·²è½¬æ¢ä¸º {format_type.upper()}: {input_path.relative_to(OUTPUT_ROOT)} â†’ {output_path.name}")
            return True
        print(f"âœ— Pandocè½¬æ¢å¤±è´¥ ({format_type}): {input_path.name} ({result.stderr.strip()})")
        return False
    except Exception as e:
        print(f"âœ— è¿è¡ŒPandocå‡ºé”™ ({format_type}) æ–‡ä»¶ {input_path.name}: {e}")
        return False

def post_process_markdown_images(md_file: Path):
    """åå¤„ç†Markdownæ–‡ä»¶ä¸­çš„å›¾ç‰‡å¼•ç”¨"""
    try:
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        replacements = [
            ('<img src="', '![]('),
            ('" class="confluence-embedded-image">', ')'),
            ('</span>', ''),
            ('<span class="confluence-embedded-file-wrapper">', '')
        ]
        for old, new in replacements:
            content = content.replace(old, new)
        
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(content)
    except Exception as e:
        print(f"âš  åå¤„ç†Markdownå›¾ç‰‡å¤±è´¥ {md_file}: {e}")

def print_summary(stats):
    """æ‰“å°è½¬æ¢æ‘˜è¦"""
    print("\n" + "="*50)
    print("ğŸ“Š è½¬æ¢æ‘˜è¦")
    print("="*50)
    stats_map = {
        'html_generated': "âœ… ç”Ÿæˆçš„HTMLæ–‡ä»¶",
        'docx_converted': "âœ… åˆ›å»ºçš„DOCXæ–‡ä»¶",
        'md_converted': "âœ… åˆ›å»ºçš„Markdownæ–‡ä»¶",
        'html_cleaned': "ğŸ§¹ æ¸…ç†çš„HTMLæ–‡ä»¶",
        'assets_copied': "ğŸ“ å¤åˆ¶çš„èµ„æºç›®å½•"
    }
    for key, label in stats_map.items():
        if stats.get(key, 0) > 0:
            print(f"{label}: {stats[key]}")
    if stats.get('errors'):
        print("\nâŒ é”™è¯¯:")
        for error in stats['errors']:
            print(f"  - {error}")
    print("="*50 + "\n")

def main():
    global EXPORT_ROOT, OUTPUT_ROOT, INDEX_HTML
    
    stats = {
        'html_generated': 0,
        'docx_converted': 0,
        'md_converted': 0,
        'html_cleaned': 0,
        'assets_copied': 0,
        'errors': []
    }
    
    parser = argparse.ArgumentParser(
        description='å°†Confluence HTMLå¯¼å‡ºè½¬æ¢ä¸ºMarkdownæˆ–DOCXæ–‡æ¡£',
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="ç¤ºä¾‹:\n"
               "  python convert_confluence.py --export-root MyExport --output-root MyOutput\n"
               "  python convert_confluence.py --type docx --export-root MyExport\n"
               "  python convert_confluence.py --skip-html --cleanup"
    )
    
    parser.add_argument('--export-root', default=DEFAULT_EXPORT_ROOT,
                      help=f'Confluence HTMLå¯¼å‡ºç›®å½• (é»˜è®¤: {DEFAULT_EXPORT_ROOT})')
    parser.add_argument('--output-root', default=DEFAULT_OUTPUT_ROOT,
                      help=f'è¾“å‡ºç›®å½• (é»˜è®¤: {DEFAULT_OUTPUT_ROOT})')
    parser.add_argument('--type', choices=['markdown', 'docx'], default=DEFAULT_OUTPUT_TYPE,
                      help=f'è¾“å‡ºæ ¼å¼ (é»˜è®¤: {DEFAULT_OUTPUT_TYPE})')
    parser.add_argument('--skip-html', action='store_true',
                      help='è·³è¿‡HTMLç”Ÿæˆé˜¶æ®µ(ä½¿ç”¨å·²æœ‰HTMLæ–‡ä»¶)')
    parser.add_argument('--cleanup', action='store_true',
                      help='è½¬æ¢å®Œæˆååˆ é™¤ä¸­é—´HTMLæ–‡ä»¶')
    
    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if len(os.sys.argv) == 1:
        parser.print_help()
        print_usage_example()
        return
    
    args = parser.parse_args()
    
    EXPORT_ROOT = Path(args.export_root)
    OUTPUT_ROOT = Path(args.output_root)
    INDEX_HTML = EXPORT_ROOT / "index.html"
    format_type = args.type
    
    # éªŒè¯å¯¼å‡ºç›®å½•
    if not EXPORT_ROOT.exists():
        print(f"âŒ é”™è¯¯: å¯¼å‡ºç›®å½•ä¸å­˜åœ¨ {EXPORT_ROOT}")
        return
    
    print(f"\n{'='*50}")
    print(f"ğŸ“ å¯¼å‡ºç›®å½•: {EXPORT_ROOT}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {OUTPUT_ROOT}")
    print(f"ğŸ”§ è¾“å‡ºæ ¼å¼: {format_type}")
    print(f"ğŸ”§ è·³è¿‡HTMLç”Ÿæˆ: {'æ˜¯' if args.skip_html else 'å¦'}")
    print(f"ğŸ§¹ æ¸…ç†HTML: {'æ˜¯' if args.cleanup else 'å¦'}")
    print(f"{'='*50}\n")
    
    print("ğŸ“ è§£æindex.html...")
    try:
        with open(INDEX_HTML, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
        nav = soup.find("ul")
        if not nav:
            raise ValueError("åœ¨index.htmlä¸­æ‰¾ä¸åˆ°<ul>å…ƒç´ ")
        page_map = walk_index(nav, Path())
    except Exception as e:
        stats['errors'].append(f"è§£æindex.htmlå¤±è´¥: {str(e)}")
        print_summary(stats)
        return

    rewritten_files = []
    if not args.skip_html:
        print("ğŸ§± é‡å†™HTMLå¹¶åˆ›å»ºç›®å½•ç»“æ„...")
        try:
            rewritten_files = build_structure_and_rewrite_links(page_map, format_type)
            stats['html_generated'] = len(rewritten_files)
            print("ğŸ–¼ï¸ å¤åˆ¶èµ„æºæ–‡ä»¶...")
            try:
                copy_asset_dirs(format_type)
                stats['assets_copied'] = len(ASSET_DIRS)
            except Exception as e:
                stats['errors'].append(f"å¤åˆ¶èµ„æºå¤±è´¥: {str(e)}")
        except Exception as e:
            stats['errors'].append(f"HTMLç”Ÿæˆå¤±è´¥: {str(e)}")
    else:
        print("â© è·³è¿‡HTMLç”Ÿæˆ...")
        for page_info in page_map.values():
            html_path = OUTPUT_ROOT / page_info['path'].with_suffix('.html')
            if html_path.exists():
                rewritten_files.append(html_path)
            else:
                stats['errors'].append(f"HTMLæ–‡ä»¶ä¸å­˜åœ¨: {html_path}")

    print(f"\nğŸ“„ è½¬æ¢ä¸º {format_type.upper()}...")
    for html_file in rewritten_files:
        try:
            if convert_to_format(html_file, format_type):
                stats[f"{'md' if format_type == 'markdown' else 'docx'}_converted"] += 1
                if args.cleanup:
                    try:
                        html_file.unlink()
                        stats['html_cleaned'] += 1
                    except Exception as e:
                        stats['errors'].append(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {html_file}: {e}")
        except Exception as e:
            stats['errors'].append(f"è½¬æ¢å¤±è´¥ {html_file}: {e}")

    print_summary(stats)

if __name__ == "__main__":
    main()
